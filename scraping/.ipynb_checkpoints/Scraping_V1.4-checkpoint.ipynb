{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Date :</strong> Created on 8 February 2021| Updated on 20 March 2021 </strong>\n",
    "\n",
    "<strong>Group 2 - Hydrogen vehicles \n",
    "    \n",
    "@author : </strong>Théo SACCAREAU\n",
    "\n",
    "<strong>scraping_V1.4\n",
    "    \n",
    "Description :</strong> The purpose of this notebook is to retrieve the desired information from the hydrogen-related articles on the Core site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install / Download / Import Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bLqSqhQE2ii7"
   },
   "outputs": [],
   "source": [
    "# Scraping librairies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Text librairy\n",
    "import re\n",
    "\n",
    "# Useful librairies\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWbEXnYJjF_h"
   },
   "source": [
    "# Part 1 - Scraping of general information (id, title, date, authors and language). \n",
    "In this first part, we will retrieve information that can be accessed directly from the search page (without the need to \"click\" on the article link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "TzKW5KC1jxmr",
    "outputId": "8e43fbdf-2ce3-47e5-f8fc-1a21cacd4928"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [15:14<00:00,  7.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a session object\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # Opening the \"df_infos.csv\" file (rather creation as it does not exist) in write mode.\n",
    "    # This file will store the scraper information (which avoids re-executing the code each time).\n",
    "    with open(\"df_infos.csv\", \"w\") as outf:\n",
    "\n",
    "        # Writing the header line.\n",
    "        outf.write(\"Id;Date;Auteur;Titre;Langue\\n\")\n",
    "\n",
    "        # Browsing the pages to retrieve information from 11450 articles (100 articles per page)\n",
    "        for page in tqdm(range(0, 11450, 100)):\n",
    "\n",
    "            # Research equation on title and abstract only (not on all the content)\n",
    "            research_eq = \"(vehicle* OR transport OR transports OR train OR trains OR tractor OR bike* \\\n",
    "            OR bicycle* OR boat OR boats OR ship OR ships OR *plane OR *planes OR aircraft* OR car \\\n",
    "            OR cars OR truck* OR lorry* OR bus OR automobile* OR motor* OR rocket*) \\\n",
    "            AND hydrogen AND (ecolog* OR climat* OR pollution* OR environment* OR \\\"renewable energy\\\" \\\n",
    "            OR emission* OR \\\"carbon neutrality\\\" OR \\\"global warming\\\" OR conservation* OR sustaina*)\"\n",
    "\n",
    "            # Data to make the request\n",
    "            request_data = {\"basicQuery\": {\n",
    "                \"count\": 100,  # 100 articles per page\n",
    "                \"searchCriteria\": \"title : (\" + research_eq + \") abstract:(\" + research_eq + \")\",\n",
    "                \"offset\": page,  # page number\n",
    "                \"sortByDate\": False}  # we sort by relevance and not by date\n",
    "            }\n",
    "\n",
    "            # Sending the POST request with the request data.\n",
    "            res = s.post(\"https://core.ac.uk/search/api/search\",\n",
    "                         json=request_data)\n",
    "\n",
    "            # Storage of results (in .json format)\n",
    "            json = res.json()\n",
    "\n",
    "            # Separation of different information :\n",
    "\n",
    "            # (1) Identifiants\n",
    "            ids = [elem['id'] for elem in json['results']]\n",
    "\n",
    "            # (2) Dates\n",
    "            dates = [elem['datePublished'][:10]\n",
    "                     if 'datePublished' in elem else '' for elem in json['results']]\n",
    "\n",
    "            # (3) Authors\n",
    "            authors = [elem['authorsString'].replace('\\n', '').replace('\\r', '').replace('/', '').replace(',', '/')\n",
    "                       if 'authorsString' in elem else '' for elem in json['results']]\n",
    "\n",
    "            # (4) Titles\n",
    "            titles = [elem['title'].replace('\\n', '').replace(';', ',').replace('\\r', '')\n",
    "                      if 'title' in elem else '' for elem in json['results']]\n",
    "\n",
    "            # (5) Language\n",
    "            languages = [elem['language']['name']\n",
    "                         if 'language' in elem else '' for elem in json['results']]\n",
    "\n",
    "            # For each of the 100 articles on the current page ... \n",
    "            for identifiant, date, author, title, language in zip(ids, dates, authors, titles, languages):\n",
    "                # ... writing to the .csv file. \n",
    "                outf.write(identifiant + ';' + date + ';' + author +\n",
    "                           ';' + title + ';' + language + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IlRAt09-t59"
   },
   "source": [
    "# Part 2 - Scraping of specific information (keywords and abstracts).\n",
    "Thanks to the recovered identifiers in part 1, we can access the content of the articles. In particular, we retrieve the keywords and summaries (if present). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MxY5BFdcXyHj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8801, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid re-executing the code from part 1 (15 min), we read the .csv file into a dataframe.\n",
    "df1 = pd.read_csv(\"df_infos.csv\", sep=';', index_col=False, encoding='utf-8')\n",
    "\n",
    "# Any duplicates are removed\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "# Format\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the duplicates are removed, only 8801 articles (out of 11450) remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBrwiE8j5_UE",
    "outputId": "5168fff9-2322-48e3-eb9d-854cee40229c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8801/8801 [2:27:42<00:00,  1.01s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Create a session object\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # Opening the \"df_content.csv\" file (rather creation as it does not exist) in write mode.\n",
    "    # This file will store the scraper information (which avoids re-executing the code each time).\n",
    "    with open(\"df_content.csv\", \"w\") as outf:\n",
    "\n",
    "        # Writing the header line.\n",
    "        outf.write(\"Id;MotCle;Abstract\\n\")\n",
    "\n",
    "        # For each identifiant\n",
    "        for id in tqdm(df1['Id']):\n",
    "\n",
    "            # Sending the request\n",
    "            url = \"https://core.ac.uk/display/\" + str(id) + \"?recSetID=\"\n",
    "            res = s.get(url)\n",
    "            soup = BeautifulSoup(res.text)\n",
    "\n",
    "            # (1) Keywords\n",
    "            try:\n",
    "                # Search for the \"Topics\" area\n",
    "                divs = soup.find('div', {'class': 'article_sum'})\n",
    "                topic = divs.find('div', {'class': None}).text.replace(\n",
    "                    \"\\r\", \"\").replace('\\n', '').strip()\n",
    "\n",
    "                # We remove the string \"Topics\" to keep only the keywords\n",
    "                topic = ' '.join(topic.split())[8:]\n",
    "\n",
    "                # The list of keywords can be separated by \";\", \",\", \".\", \"and\" or \"AND\".\n",
    "                if (';' in topic and ',' not in topic):\n",
    "                    topic = topic.split(';')\n",
    "                elif (',' in topic and ';' not in topic):\n",
    "                    topic = topic.split(',')\n",
    "                elif (';' in topic and ',' in topic):\n",
    "                    tempo = topic.split(';')\n",
    "                    topic = []\n",
    "                    for i in tempo:\n",
    "                        topic.extend(i.split(','))\n",
    "                elif ('.' in topic):\n",
    "                    topic = topic.split('.')\n",
    "                elif (' - ' in topic):\n",
    "                    topic = topic.split(' - ')\n",
    "                elif ('and' in topic):\n",
    "                    topic = topic.split('and')\n",
    "                elif ('AND' in topic):\n",
    "                    topic = topic.split('AND')\n",
    "                else:\n",
    "                    topic = [topic]\n",
    "\n",
    "                # In addition, some keywords can be in the summary (at the end with the mention \"Keywords: \")\n",
    "                abstract = soup.find('p',  {'class': 'abstract'})\n",
    "                if (abstract is not None):\n",
    "                    abstract = abstract.text.strip().replace(\"\\r\", \"\").replace('\\n', '')\n",
    "                    if (\"Keywords\" in abstract):\n",
    "                        index = abstract.rfind(\"Keywords:\") + len(\"Keywords:\")\n",
    "                        keywords = abstract[index:].split(',')\n",
    "                        topic.extend(keywords)\n",
    "\n",
    "            # If the \"topics\" area does not exist, we search only in the abstracts \n",
    "            except:\n",
    "                topic = []\n",
    "                abstract = soup.find('p',  {'class': 'abstract'})\n",
    "                \n",
    "                if (abstract is not None):\n",
    "                    abstract = abstract.text.strip().replace(\"\\r\", \"\").replace('\\n', '')\n",
    "\n",
    "                    # The keywords can be :\n",
    "                    # - either at the end of the summary preceded by the words \"Keywords\"; or\n",
    "                    if (\"Keywords\" in abstract):\n",
    "                        index = abstract.rfind(\"Keywords:\") + len(\"Keywords:\")\n",
    "                        keywords = abstract[index:].split(',')\n",
    "                        topic.extend(keywords)\n",
    "\n",
    "                    # - or at the end of the summary in a list (in the last sentences)\n",
    "                    else:\n",
    "                        index = abstract.rfind('.') + 1\n",
    "                        \n",
    "                        # The list of keywords can be separated by \";\"\n",
    "                        if (';' in abstract[index:]):\n",
    "                            abstract_pv = abstract[index:].split(';')\n",
    "                            topic = [topic for topic in abstract_pv]\n",
    "\n",
    "                        # or the list of keywords can be separated by \",\"\n",
    "                        elif (',' in abstract[index:]):\n",
    "                            abstract_v = abstract[index:].split(',')\n",
    "\n",
    "                            # # The \",\" may be present in a normal sentence, does not mean that it is necessarily a list.\n",
    "                            isTopics = True\n",
    "                            for i in abstract_v:\n",
    "                                if (len(i) >= 40):\n",
    "                                    isTopics = False\n",
    "                            if (isTopics):\n",
    "                                topic = [topic for topic in abstract_v]\n",
    "\n",
    "            # (2) Abstract\n",
    "            try:\n",
    "                # We check that there is a summary.\n",
    "                abstract = soup.find('p',  {'class': 'abstract'}).text.strip().replace(\n",
    "                    \"\\r\", \"\").replace('\\n', '').replace(';', ',')\n",
    "            except:\n",
    "                # If not, we return an empty string.\n",
    "                abstract = ''\n",
    "\n",
    "            # The list of keywords is combined into a single string separated by '/'.\n",
    "            topic = [word for word in topic if len(word) > 3]\n",
    "            chaine = \"\"\n",
    "            for i in topic:\n",
    "                chaine = chaine + i + \"/\"\n",
    "\n",
    "            # Writing to the file \n",
    "            outf.write(str(id) + ';' + chaine + ';' + abstract + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvj1kRZEG3lP"
   },
   "source": [
    "# Part 3 - Extraction of information from abstracts and titles\n",
    "In part 1, we have collected the titles of the articles and in part 2, the abstracts. From this information, we will look for the vehicles, fields, technologies, etc. mentioned in the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "qsyapABJmpTo",
    "outputId": "3100d1d5-29c2-4d32-a71f-649ca5bcd70f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8439, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid re-executing the code from part 1 (15 min), we read the .csv file into a dataframe.\n",
    "df2 = pd.read_csv(\"df_content.csv\", sep=';', index_col=False, encoding='utf-8')\n",
    "\n",
    "# Any duplicates are removed\n",
    "df2 = df2.drop_duplicates()\n",
    "\n",
    "# Articles that do not have an abstract are not kept\n",
    "df2 = df2.dropna(subset=['Abstract'])\n",
    "\n",
    "# Format\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deleting the articles without an abstract, only 8439 articles remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8439, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A join is made between the two dataframes.\n",
    "# The join concerns the identifiers (only column in common between the two dataframes).\n",
    "# The 'df' dataframe will contain all the information needed to populate the database. \n",
    "df = pd.merge(df1, df2)\n",
    "\n",
    "# Any duplicates are removed\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Format\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we will work on a corpus of 8439 articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6KfhwIF4dloU"
   },
   "outputs": [],
   "source": [
    "# Titles and abstracts are extracted into two variables. \n",
    "# To make it easier to find information, we put everything in lower case. \n",
    "# This avoids having to differentiate between \"Car\" and \"car\". \n",
    "title = df['Titre'].apply(lambda x : x if (x is np.nan) else x.lower())\n",
    "abstract = df['Abstract'].apply(lambda x : x if (x is np.nan) else x.lower())\n",
    "\n",
    "# Now that the abstracts are temporarily stored in a variable, they can be deleted from\n",
    "# the dataframe (they will not be stored in the database). \n",
    "df = df.drop(columns=['Abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7LKYnuV9O5_"
   },
   "source": [
    "## Part 3-1) An attribute is associated with a single vocabulary word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TX5tzgs3o7aN"
   },
   "outputs": [],
   "source": [
    "def extraction_info1(list_att_BD: list, list_voc : list, titles: pd.Series, abstracts: pd.Series) -> list:\n",
    "    \"\"\" Documentation :\n",
    "            - Function that checks for each article whether the summary or title contains a word \n",
    "            from the vocabulary list passed in parameter. \n",
    "\n",
    "        Parameters: \n",
    "            - list_att_BD : list of attributes that will be inserted in the database. \n",
    "            - list_voc : list of vocabulary to be searched in the abstracts or titles. \n",
    "            - titles : titles of articles \n",
    "            - abstracts : abtracts of the articles \n",
    "        \n",
    "        Output : \n",
    "            - res : list containing for each article the list of attributes extracted from the abstract or title. \n",
    "\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    # The list of titles and summaries is browsed simultaneously. \n",
    "    for title, abstract in zip(tqdm(titles), abstracts):\n",
    "        try:\n",
    "            # The list of all the attributes contained in the summary is retrieved in \"list_abs\". \n",
    "            list_abs = [item_BD for item, item_BD in zip(\n",
    "                list_voc, list_att_BD) if re.findall(item, abstracts) != []]\n",
    "        except:\n",
    "            # If no match, empty list \n",
    "            list_abs = []\n",
    "        try:\n",
    "            # Ditto for titles\n",
    "            list_tit = [item_BD for item, item_BD in zip(\n",
    "                list_voc, list_att_BD) if re.findall(item, title) != []]\n",
    "        except:\n",
    "            # If no match, empty list \n",
    "            list_tit = []\n",
    "\n",
    "        # The results of the two lists are concatenated. \n",
    "        list_res = list_abs + list_tit\n",
    "\n",
    "        # Any duplicates are removed \n",
    "        list_res = list(set(list_res))  \n",
    "\n",
    "        # The list of attributes is combined into a single string separated by '/'.\n",
    "        chaine_res = \"\"\n",
    "        for item in list_res:\n",
    "            chaine_res = chaine_res + item + \"/\"\n",
    "\n",
    "        # We add this string to the final \"res\" list\n",
    "        res.append(chaine_res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-1) Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUYaPCwMnkj8",
    "outputId": "92d7ce0f-fdc2-4a99-f290-c21c2b759e23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:02<00:00, 3162.19it/s]\n"
     ]
    }
   ],
   "source": [
    "list_countries = [\"algeria\", \"angola\", \"(benin|dahomey)\", \"botswana\", \"burkina\", \"burundi\", \"cameroon\", \"cape verde\", \"central african republic\", \"( |,|\\.|\\-)chad( |,|\\.|\\-)\", \"comoros\", \"( |,|\\.|\\-)congo( |,|\\.|\\-)\", \"djibouti\",\n",
    "                  \"egypt\", \"equatorial guinea\", \"eritrea\", \"ethiopia\", \"( |,|\\.|\\-)gabon( |,|\\.|\\-)\", \"gambia\", \"ghana\", \"guinea\", \"guinea(\\-| )bissau\", \"ivory coast\", \"kenya\", \"lesotho\", \"liberia\", \"libya\",\n",
    "                  \"madagascar\", \"malawi\", \"( |,|\\.|\\-)mali( |,|\\.|\\-)\", \"mauritania\", \"mauritius\", \"morocco\", \"mozambique\", \"namibia\", \"( |,|\\.|\\-)niger( |,|\\.|\\-)\", \"nigeria\", \"rwanda\", \"sao tome and principe\", \"senegal\",\n",
    "                  \"seychelles\", \"sierra leone\", \"somalia\", \"south(\\-| )africa\", \"( |,|\\.|\\-)sudan( |,|\\.|\\-)\", \"swaziland\", \"tanzania\", \"( |,|\\.|\\-)togo( |,|\\.|\\-)\", \"tunisia\", \"uganda\", \"zambia\", \"zimbabwe\", \"albania\", \"andorra\",\n",
    "                  \"armenia\", \"austria\", \"azerbaijan\", \"belarus\", \"belgium\", \"bosnia\", \"bulgaria\", \"croatia\", \"cyprus\", \"(czech republic|czechia)\", \"denmark\", \"estonia\", \"finland\",\n",
    "                  \"france\", \"georgia\", \"germany\", \"greece\", \"hungary\", \"iceland\", \"ireland\", \"italy\", \"latvia\", \"liechtenstein\", \"lithuania\", \"luxembourg\",\n",
    "                  \"macedonia\", \"( |,|\\.|\\-)malta( |,|\\.|\\-)\", \"moldova\", \"monaco\", \"montenegro\", \"netherlands\", \"( |,|\\.|\\-)norway( |,|\\.|\\-)\", \"poland\", \"portugal\", \"romania\", \"san marino\", \"serbia\", \"slovakia\", \"slovenia\",\n",
    "                  \"(espana|spain)\", \"sweden\", \"switzerland\", \"ukraine\", \"(united(\\-| )kingdom|england|scotland|wales|northern ireland)\", \"vatican city\", \"antigua and barbuda\", \"bahamas\", \"barbados\", \"belize\", \"canada\",\n",
    "                  \"costa rica\", \"( |,|\\.|\\-)cuba( |,|\\.|\\-)\", \"dominica( |,|\\.|\\-)\", \"dominican republic\", \"el salvador\", \"grenada\", \"guatemala\", \"haiti\", \"honduras\", \"jamaica\", \"mexico\", \"nicaragua\", \"panama\",\n",
    "                  \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\", \"trinidad and tobago\", \"(( |,|\\.|\\-)us( |,|\\.|\\-)|usa|united(\\-| )states)\", \"argentina\", \"bolivia\", \"brazil\", \"( |,|\\.|\\-)chile( |,|\\.|\\-)\",\n",
    "                  \"colombia\", \"ecuador\", \"guyana\", \"paraguay\", \"( |,|\\.|\\-)peru( |,|\\.|\\-)\", \"suriname\", \"uruguay\", \"venezuela\", \"afghanistan\", \"bahrain\", \"bangladesh\", \"bhutan\", \"brunei\", \"(burma|myanmar)\",\n",
    "                  \"cambodia\", \"china\", \"(east timor|timor(\\-| )leste)\", \"india\", \"indonesia\", \"( |,|\\.|\\-)iran( |,|\\.|\\-)\", \"( |,|\\.|\\-)iraq( |,|\\.|\\-)\", \"israel\", \"japan\", \"( |,|\\.|\\-)jordan( |,|\\.|\\-)\", \"kazakhstan\",\n",
    "                  \"(republic of korea|south(\\-| )korea|korea, south)\", \"(north(\\-| )korea|korea, north)\", \"kuwait\", \"kyrgyzstan\", \"laos\", \"lebanon\", \"malaysia\", \"maldives\", \"mongolia\",\n",
    "                  \"( |,|\\.|\\-)nepal( |,|\\.|\\-)\", \"( |,|\\.|\\-)oman( |,|\\.|\\-)\", \"pakistan\", \"palestine\", \"philippines\", \"qatar\", \"russia\", \"saudi arabia\", \"singapore\", \"sri lanka\", \"syria\", \"tajikistan\", \"thailand\", \"turkey\",\n",
    "                  \"turkmenistan\", \"united arab emirates\", \"uzbekistan\", \"vietnam\", \"yemen\", \"australia\", \"fiji\", \"kiribati\", \"marshall islands\", \"micronesia\", \"( |,|\\.|\\-)nauru( |,|\\.|\\-)\", \"new(\\-| )zealand\",\n",
    "                  \"palau( |,|\\.|\\-)\", \"papua new guinea\", \"samoa\", \"solomon islands\", \"tonga\", \"tuvalu\", \"vanuatu\", \"democratic republic of congo\"]\n",
    "\n",
    "list_countries_BD = [\"Algérie\", \"Angola\", \"Bénin\", \"Botswana\", \"Burkina Faso\", \"Burundi\", \"Cameroun\", \"Cap vert\",\n",
    "                     \"République centrafricaine\", \"Tchad\", \"Comores\", \"Congo\", \"Djibouti\", \"Egypte\",\n",
    "                     \"Guinée équatoriale\", \"Erythree\", \"Ethiopie\", \"Gabon\", \"Gambie\", \"Ghana\", \"Guinée\",\n",
    "                     \"Guinée-Bissau\", \"Côte d'Ivoire\", \"Kenya\", \"Lesotho\", \"Liberia\", \"Libye\", \"Madagascar\",\n",
    "                     \"Malawi\", \"Mali\", \"Mauritanie\", \"Ile Maurice\", \"Maroc\", \"Mozambique\", \"Namibie\", \"Niger\",\n",
    "                     \"Nigeria\", \"Rwanda\", \"Sao Tomé-et-Principe\", \"Sénégal\", \"Seychelles\", \"Sierra Leone\",\n",
    "                     \"Somalie\", \"Afrique du Sud\", \"Soudan\", \"Swaziland\", \"Tanzanie\", \"Togo\", \"Tunisie\", \"Ouganda\",\n",
    "                     \"Zambie\", \"Zimbabwe\", \"Albanie\", \"Andorre\", \"Arménie\", \"Autriche\", \"Azerbaijan\",\n",
    "                     \"Biélorussie\", \"Belgique\", \"Bosnie\", \"Bulgarie\", \"Croatie\", \"Chypre\", \"République Tchèque\",\n",
    "                     \"Danemark\", \"Estonie\", \"Finlande\", \"France\",\n",
    "                     \"Géorgie\", \"Allemagne\", \"Grèce\", \"Hongrie\", \"Islande\", \"Irelande\", \"Italie\", \"Lettonie\",\n",
    "                     \"Liechtenstein\", \"Lituanie\", \"Luxembourg\", \"Macédoine\", \"Malte\", \"Moldavie\", \"Monaco\",\n",
    "                     \"Montenegro\", \"Pays-Bas\", \"Norvège\", \"Pologne\", \"Portugal\", \"Roumanie\", \"Saint-Marin\",\n",
    "                     \"Serbie\", \"Slovaquie\", \"Slovénie\", \"Espagne\", \"Suède\", \"Suisse\", \"Ukraine\", \"Royaume-Uni\",\n",
    "                     \"Vatican\", \"Antigua-et-Barbuda\", \"Bahamas\", \"La Barbade\", \"Belize\", \"Canada\", \"Costa Rica\",\n",
    "                     \"Cuba\", \"Dominique\", \"République dominicaine\", \"El Salvador\", \"Grenade\", \"Guatemala\", \"Haiti\",\n",
    "                     \"Honduras\", \"Jamaique\", \"Mexique\", \"Nicaragua\", \"Paname\", \"Saint-Christophe et Niévès\",\n",
    "                     \"Saint Lucie\", \"Saint Vincent et les Grenadines\", \"Trinidad et Tobago\", \"Etats-Unis\",\n",
    "                     \"Argentine\", \"Bolivie\", \"Brésil\", \"Chili\", \"Colombie\", \"Equateur\", \"Guyane\", \"Paraguay\",\n",
    "                     \"Pérou\", \"Suriname\", \"Uruguay\", \"Venezuela\", \"Afghanistan\", \"Bahrein\", \"Bangladesh\",\n",
    "                     \"Bhoutan\", \"Brunei\", \"Myanmar\", \"Cambodge\", \"Chine\", \"Timor oriental\", \"Inde\", \"Indonésie\",\n",
    "                     \"Iran\", \"Irak\", \"Israël\", \"Japon\", \"Jordanie\", \"Kazakhstan\", \"Corée du Sud\", \"Corée du Nord\",\n",
    "                     \"Koweït\", \"Kyrgyzstan\", \"Laos\", \"Liban\", \"Malaysie\", \"Maldives\", \"Mongolie\", \"Népal\", \"Oman\",\n",
    "                     \"Pakistan\", \"Palestine\", \"Philippines\", \"Qatar\", \"Russie\", \"Arabie Saoudite\", \"Singapour\",\n",
    "                     \"Sri Lanka\", \"Syrie\", \"Tajikistan\", \"Thaïlande\", \"Turquie\", \"Turkmenistan\",\n",
    "                     \"Emirats Arabes Unis\", \"Ouzbekistan\", \"Viêtnam\", \"Yémen\", \"Australie\", \"Fiji\",\n",
    "                     \"République de Kiribati\", \"Iles Marshall\", \"Micronésie\", \"Nauru\", \"Nouvelle-Zélande\", \"Palau\",\n",
    "                     \"Papouasie Nouvelle Guinée\", \"Iles Samoa\", \"Iles Salomon\", \"Tonga\", \"Tuvalu\", \"Vanuatu\",\n",
    "                     \"République Democratique du Congo\"]\n",
    "\n",
    "df['Pays'] = extraction_info1(\n",
    "    list_countries_BD, list_countries, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-2) Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3i3ICQXKkEcy",
    "outputId": "071a1395-ba76-404d-ac35-f2f04cb8b618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:00<00:00, 22629.95it/s]\n"
     ]
    }
   ],
   "source": [
    "list_vehicles = [\" train\", \" tractor\", \" (bike|bicycle)\", \" (boat|ship|vessel|ferry|narrowboat|barge|liner|yacht)\",\n",
    "                 \" (aeroplane| plane|airplane|aircraft| jet)\", \" (car|automobile|saloon|van|pickup)\",\n",
    "                 \" (truck|lorry|semitrailer|rig|juggernaut|hgv|heavy goods vehicle)\", \" (bus|coach)\", \" rocket\",\n",
    "                 \" helicopter\", \"(( | motor)bike| motorcycle)\", \" (tram|streetcar)\", \"submarine\"]\n",
    "# We want a space (end of word), a comma, an s or a full stop after our words.\n",
    "list_vehicles = [vehi + \"( |s|,|\\.)\" for vehi in list_vehicles]\n",
    "\n",
    "list_vehicles_BD = [\"Train\", \"Tracteur\", \"Vélo\", \"Bateau\", \"Avion\", \"Voiture\", \"Camion\", \"Bus\", \"Fusée\",\n",
    "                    \"Hélicoptère\", \"Moto\", \"Tramway\", \"Sous-marin\"]\n",
    "\n",
    "df['Vehicule'] = extraction_info1(\n",
    "    list_vehicles_BD, list_vehicles, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-3) Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nZgP2zV81YF",
    "outputId": "2445bcb7-7a01-4573-8557-f29371c42536"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:00<00:00, 10827.96it/s]\n"
     ]
    }
   ],
   "source": [
    "list_brands = [\"hyundai\", \"toyota\", \"renault\", \"honda\", \"airbus\", \"boeing\", \"thales\", \"mercedes\",\n",
    "               \"( |')audi( |'|\\.|,)\", \"( |'|\\.|,)kia( |'|\\.|,)\", \"river( |\\-|)simple\", \"nissan\",\n",
    "               \"( |'|\\.|,)ford( |'|\\.|,)\",\n",
    "               \"daimler\", \"alstom\", \"bmw\", \"hopium\", \"peugeot\", \"volkswagen\", \"general motors\", \"( |'|\\.|,)psa( |'|\\.|,)\",\n",
    "               \"(roland gumpert|apollo automobil|gmbh)\", \"mazda\", \"aston martin\", \"pininfarina\", \"suzuki\",\n",
    "               \"volvo\", \"( |'|\\.|,)opel( |'|\\.|,)\", \"dassault\", \"cessna\", \"bombardier\", \" mig( |'|\\.|,)\",\n",
    "               \"diamond aircraft\", \"zeroavia\", \"rolls-royce\", \"( |'|\\.|,)eviation( |'|\\.|,)\", \"(gknpz|khrounitchev)\",\n",
    "               \"spacex\", \"avio( |'|\\.|,)\", \"ariane\", \"united launch alliance\", \"mcdonnell douglas\", \"mitsubishi\",\n",
    "               \"isro\", \"ioujnoïe\", \"citroen\", \"( |'|\\.|,)fiat( |'|\\.|,)\", \"( |'|\\.|,)lancia( |'|\\.|,)\", \"skoda\",\n",
    "               \"yamaha\", \"( |'|\\.|,)ktm( |'|\\.|,)\", \"kawasaki\", \"( |'|\\.|,)ducati( |'|\\.|,)\", \"suzuki\"]\n",
    "\n",
    "list_brands_BD = [\"Hyundai\", \"Toyota\", \"Renault\", \"Honda\", \"Airbus\", \"Boeing\", \"Thalès\", \"Mercedes\", \"Audi\", \"Kia\",\n",
    "                  \"Riversimple\", \"Nissan\", \"Ford\", \"Daimler\", \"Alstom\", \"BMW\", \"Hopium\", \"Peugeot\", \"Volkswagen\",\n",
    "                  \"General Motors\", \"PSA\", \"Roland Gumpert\", \"Mazda\", \"Aston Martin\", \"Pininfarina\", \"Suzuki\",\n",
    "                  \"Volvo\", \"Opel\", \"Dassault\", \"Cessna\", \"Bombardier\", \"MiG\", \"Diamond Aircraft\", \"ZeroAvia\",\n",
    "                  \"Rolls-Royce\", \"Eviation\", \"Khrounitchev\", \"SpaceX\", \"Avio\", \"ArianeGroup\",\n",
    "                  \"United Launch Alliance\", \"McDonnell Douglas\", \"Mitsubishi Heavy Industries\", \"ISRO\",\n",
    "                  \"Ioujnoie\", \"Citroën\", \"Fiat\", \"Lancia\", \"Skoda\", \"Yamaha\", \"KTM\", \"Kawasaki\", \"Ducati\",\n",
    "                  \"Suzuki\"]\n",
    "\n",
    "df['Marque'] = extraction_info1(list_brands_BD, list_brands, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY43fRK_9UPD"
   },
   "source": [
    "## Part 3-2) An attribute is associated with several vocabulary words.\n",
    "For instance : \"militar\", \"army\", \"soldie\", \"warfare\", \"armed forces\" for the domain \"military\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OZ1MRLokvtNA"
   },
   "outputs": [],
   "source": [
    "def extraction_info2(list_att_BD: list, list_list_voc: list, titles: pd.Series, abstracts: pd.Series) -> list:\n",
    "    \"\"\" Documentation :\n",
    "            - Function that checks for each article whether the summary or title contains a word \n",
    "            from the vocabulary list passed in parameter. \n",
    "            The difference with the previous function is that here an attribute is associated with several vocabulary words. \n",
    "\n",
    "\n",
    "        Parameters: \n",
    "            - list_att_BD : list of attributes that will be inserted in the database. \n",
    "            - list_list_voc : list containing the vocabulary lists to be searched in the abstracts or titles. \n",
    "            - titles : titles of articles \n",
    "            - abstracts : abtracts of the articles \n",
    "\n",
    "        Output : \n",
    "            - res : list containing for each article the list of attributes extracted from the abstract or title. \n",
    "\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    # The list of titles and summaries is browsed simultaneously.\n",
    "    for title, abstract in zip(tqdm(titles), abstracts):\n",
    "\n",
    "        # Each vocabulary list associated with an attribute is browsed\n",
    "        list_tempo = []\n",
    "        for att_BD, list_voc in zip(list_att_BD, list_list_voc):\n",
    "\n",
    "            # # The list of all the attributes contained in the summary is retrieved in \"list_abs\".\n",
    "            try:\n",
    "                list_abs = [\n",
    "                    voc for voc in list_voc if re.findall(voc, abstract) != []]\n",
    "            except:\n",
    "                list_abs = []\n",
    "\n",
    "            # Ditto for titles\n",
    "            try:\n",
    "                list_tit = [\n",
    "                    voc for voc in list_voc if re.findall(voc, title) != []]\n",
    "            except:\n",
    "                list_tit = []\n",
    "\n",
    "            # The results of the two lists are concatenated.\n",
    "            list_res = list_abs + list_tit\n",
    "\n",
    "            # If the list contains at least one item then the attribute is mentioned in the summary or title\n",
    "            if (len(list_res) > 0):\n",
    "                list_tempo.append(att_BD)\n",
    "\n",
    "        # The list of attributes is combined into a single string separated by '/'.\n",
    "        chaine_res = \"\"\n",
    "        for item in list_tempo:\n",
    "            chaine_res = chaine_res + item + \"/\"\n",
    "\n",
    "        # We add this string to the final \"res\" list\n",
    "        res.append(chaine_res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-1) Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5l9_FF4urD96",
    "outputId": "db08e8bf-9dbc-46b3-beea-fc2df505b75d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:01<00:00, 5362.21it/s]\n"
     ]
    }
   ],
   "source": [
    "combustion_int = [\"internal( |\\-)combustion engine\", \"i.c.e\", \"burn(s| ) * fuel * cylinder(s| )\",\n",
    "                  \"reciprocating engine\", \"piston engine\"]\n",
    "\n",
    "reaction = [\"jet (engine|propulsion)\", \"turbojet\", \"propellant \", \"projection * fluid\", \"turbofan\",\n",
    "            \"ramjet\", \"pulse jet\"]\n",
    "\n",
    "pile_combustible = [\"fuel cell\", \"electrochemical cell\", \"oxidi(z|s)ation\", \"oxidizing agent\",\n",
    "                    \"redox\", \"chemical energy\", \"anode\", \"cathode\", \"electrolyte\"]\n",
    "\n",
    "hybride = [\"hybrid\", \"hydrogen addition on\"]\n",
    "\n",
    "list_engines = [combustion_int, reaction, pile_combustible, hybride]\n",
    "list_engines_BD = [\"Combustion interne\",\n",
    "                   \"Réaction\",  \"Pile à combustible\", \"Hybride\"]\n",
    "\n",
    "df['Moteur'] = extraction_info2(list_engines_BD, list_engines, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-2) Storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-NafIIDbpmF",
    "outputId": "f1896db9-80f0-4be8-9055-fd5275b115c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:01<00:00, 5362.01it/s]\n"
     ]
    }
   ],
   "source": [
    "gaz = [\"compress\", \"gaseous\", \"increase * pressure\", \"high( |-)pressure\", \"ch2\", \"cgh2\",\n",
    "       \"under( |-)pressure\", \"storage density\", \"hydrogen tanks\"]\n",
    "\n",
    "liquide = [\"liqu(e|i)f\", \"liquid (hydrogen|state|form)\", \"LH2\", \"cooled \", \"critical point\", \"33( |)k\",\n",
    "           \"-253(| )°c\", \"252(,|.)87\", \"20(.|,)28( |)k\", \"parahydrogen\", \"orthohydrogen\", \"low temperature\",\n",
    "           \"cooling\", \"chilled\"]\n",
    "\n",
    "solide = [\"solid (hydrogen|state|form)\", \"a(b|d)sorption\",\n",
    "          \"metal alloy\", \"hydride\", \"alanate\"]\n",
    "\n",
    "list_storages = [gaz, liquide, solide]\n",
    "list_storages_BD = ['Gaz', 'Liquide', 'Solide']\n",
    "\n",
    "df['Stockage'] = extraction_info2(\n",
    "    list_storages_BD, list_storages, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-3) Types of production "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mw51kFSE2i9_",
    "outputId": "7fa3f29c-50a8-4cae-8e2d-c610c727c2a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:04<00:00, 1767.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# (1) Hydrocarbons\n",
    "SMR = [\"smr\", \"steam (methane |)reforming\", \"nickel catalyst\", \"endothermic reaction\", \"iron oxide\",\n",
    "       \"reaction * (hydrocarbons|fuels) * water\"]\n",
    "\n",
    "methane_pyrolysis = [\"pyrolysis\", \"bubble column\",\n",
    "                     \"molten metal catalyst\", \"solid carbon\"]\n",
    "\n",
    "partial_oxidation = [\"partial oxidation\", \"( |\\()pox( |\\)|,|\\.)\", \"substoichiometric\",\n",
    "                     \"(fuel\\-air|fuel\\-oxygen) mixture\", \"water-gas shift reaction\",\n",
    "                     \"partial. combusted\", \"partial oxidation reactor\"]\n",
    "\n",
    "plasma_reforming = [\"plasma reforming\", \"kvaerner\", \"cb&h\", \"carbon black\", \"plasma arc waste disposal\",\n",
    "                    \"plasma gasification\", \"plasma converter\"]\n",
    "\n",
    "coal_gasification = [\"coal gasification\", \"break molecular bonds in coal\", \"coal * water * (air|oxygen)\",\n",
    "                     \"gaseous mix of hydrogen and carbon monoxide\"]\n",
    "\n",
    "# (2) Water\n",
    "water_electrolysis = [\"electrolysis\", \"(split|decompose) water\", \"water (splitting|decomposition)\",\n",
    "                      \"potential difference\", \"electrolyser\", \"electrolyte\", \"polymer\", \"amkaline\"]\n",
    "\n",
    "electrochemically = [\"electrochemically\", \"(methanol|ethanol|formic acid[glycerol) * electrolys\",\n",
    "                     \"sulfur-iodine cycle\", \"s-i cycle\", \"sulfur * iodine\", ]\n",
    "\n",
    "radiolysis = [\"radiolysis\", \"radiolytically\"]\n",
    "\n",
    "thermolysis = [\"thermolysis\"]\n",
    "\n",
    "thermochemical = [\"thermochemical\", \"heat sources * chemical reactions\"]\n",
    "\n",
    "ferrosilicon = [\"ferrosilicon\"]\n",
    "\n",
    "algae = [\"photobiological (water|) splitting\", \"algae\",\n",
    "         \"photobioreactor\", \"photosynthesis\"]\n",
    "\n",
    "photocatalytic = [\"photocatalytic\", \"solar energy to hydrogen\", \"photoelectrochemical cell\",\n",
    "                  \"artificial photosynthesis\"]\n",
    "\n",
    "# (3) Biohydrgen\n",
    "fermentative = [\"bioreactor\", \"bacteria\", \"fermentative\", \"organic (substrate|compound)\", \"biohydrogen\",\n",
    "                \"fermentation\"]\n",
    "\n",
    "enzymatic = [\"enzyma\", \"sugars\"]\n",
    "\n",
    "biocatalysed_electrolysis = [\"electrohydrogenesis\", \"microbial fuel cell\", \"biocatalysed electrolysis\",\n",
    "                             \"electrohydrogenesis\", \"electrolysis * microbes\"]\n",
    "\n",
    "\n",
    "list_type_prod = [SMR, methane_pyrolysis, partial_oxidation, plasma_reforming, coal_gasification,\n",
    "                 water_electrolysis, electrochemically, radiolysis, thermolysis, thermochemical,\n",
    "                 ferrosilicon, algae, photocatalytic]\n",
    "list_type_prod_BD = [\"SMR\", \"Pyrolyse du méthane\", \"Oxydation partielle\", \"Reformage plasma\",\n",
    "                     \"Gazéification du charbon\", \"Electrolyse\", \"Electrochimie\", \"Radiolyse\", \"Thermolyse\",\n",
    "                     \"Thermochimie\", \"Ferrosilicium\", \"Culture d'algues\", \"Fission photocatalytique\",\n",
    "                     \"Fermentation\", \"Production enzymatique\", \"Electrolyse biocatalysée\"]\n",
    "\n",
    "\n",
    "df['Production'] = extraction_info2(\n",
    "    list_type_prod_BD, list_type_prod, title, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-4) Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oucGQISln9Pp",
    "outputId": "59c68b79-dc91-46f0-b34e-801550143242"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:15<00:00, 549.49it/s]\n"
     ]
    }
   ],
   "source": [
    "militaire = [\"militar\", \"army\", \"soldie\", \"warfare\", \"armed forces\",\n",
    "             \"navy\", \"air force\", \"nato\", \"defence\", \"regiment\", \"fighting\"]  # NATO\n",
    "\n",
    "agriculture = [\"agricultur\", \"cultivat\", \"agronomy\", \"harvest\", \"crops\", \"tractor\", \"sowing\", \"seeding\",\n",
    "               \"gmo\", \"harvest\", \"plough\", \"plow\", \"irrigation\", \"insecticid\", \"herbicide\",\n",
    "               \"weedkiller\", \"seed\", \"farm\", \"fertilizer\", \"fertiliser\", \"cultur\"]\n",
    "\n",
    "astronautique = [\" space\", \"astronautics\", \"orbit\", \"atmosphere\", \"space shuttle\", \"rocket\", \"landing\",\n",
    "                 \"aerodynamic\", \"propulsion\", \"thrust\", \"cosmonaut\", \"jet engine\", \"thales\", \"boeing\",\n",
    "                 \"take-off \", \"takeoff \", \"(gknpz|khrounitchev)\", \"( |'|\\.|,)nasa( |'|\\.|,)\", \"spacex\",\n",
    "                 \"avio( |'|\\.|,)\", \"ariane\", \"united launch alliance\", \"mcdonnell douglas\",\n",
    "                 \"isro\", \"ioujnoïe\"]\n",
    "\n",
    "aéronautique = [\"(aeroplane| plane|airplane)\", \"aircraft\", \"aeronautic\", \"aviation\", \"helicopter\",\n",
    "                \"aerodynamic\", \"boeing\", \"airbus\", \"(transonic|(in|)compressible) flow\",\n",
    "                \"take-off \", \"takeoff \", \"thales\", \"dassault\", \"cessna\", \"bombardier\"]\n",
    "\n",
    "transport_march = [\"charter\", \" (truck|lorry|semitrailer|rig|juggernaut|hgv|heavy goods vehicle)( |s|,|\\.)\",\n",
    "                   \" trailer\", \" container\", \" freight\", \" cargo\", \" shipment\", \" merchant ship\", \"service road\",\n",
    "                   \" logistics\", \" handling\", \" hefting\", \" merchandise\", \" goods\", \" transporter\", \" haulier\",\n",
    "                   \"long-haul * transportation\", \"haulage contractor\", \" carr(ying|ier)\", \"wagon\"]\n",
    "\n",
    "transport_pers = [\"(public|mass|urban|school) (transport|transit)\", \" passenger\", \" occupant\", \" traverl(l|)er\",\n",
    "                  \" voyager\", \"carrriage\", \" (bike|bicycle|yacht|ferry|jet|tram|streetcar|( | motor)bike| motorcycle|\\\n",
    "                         car|automobile|saloon|van|pickup|bus|coach|underground|subwat|metro)( |s|,|\\.)\", ]\n",
    "\n",
    "politique = [\" decree( |s|,|\\.)\", \" law( |s|,|\\.)\", \" legislation\", \"carbon tax\", \"paris agreement\", \"unfccc\", \" cop\",\n",
    "             \" protocol\", \"cmp [0-9]\", \"cop [0-9]\", \"climate change conference\", \"subsidy\"]\n",
    "\n",
    "automobile = [\"automobile\", \" (saloon|van|pickup|car)( |s|,|\\.)\", \" motoring( |s|,|\\.)\", \"tailpipe\",\n",
    "              \"exhaust pipe\", \"horsepower\", \"dealership\", \"motor rac(ing|e)\", \"hyundai\", \"toyota\",\n",
    "              \"renault\", \"honda\", \"mercedes\", \"( |')audi( |'|\\.|,)\", \"( |'|\\.|,)kia( |'|\\.|,)\",\n",
    "              \"river( |\\-|)simple\", \"nissan\", \"( |'|\\.|,)ford( |'|\\.|,)\",\n",
    "              \"daimler\", \"bmw\", \"hopium\", \"peugeot\", \"volkswagen\", \"general motors\", \"( |'|\\.|,)psa( |'|\\.|,)\",  \"(roland gumpert|apollo automobil|gmbh)\",\n",
    "              \"mazda\", \"aston martin\", \"pininfarina\", \"suzuki\", \"volvo\", \"( |'|\\.|,)opel( |'|\\.|,)\", \"rolls-royce\", \"citroen\", \"( |'|\\.|,)fiat( |'|\\.|,)\",\n",
    "              \"( |'|\\.|,)lancia( |'|\\.|,)\", \"skoda\", \"yamaha\", \"( |'|\\.|,)ktm( |'|\\.|,)\", \"kawasaki\", \"ducati\", \"suzuki\"]\n",
    "\n",
    "ferroviaire = [\" rail\", \" train( |s|,|\\.)\", \"alstom\", \"locomotive\",\n",
    "               \"interurbain\", \"wagon\", \"freight car\", \"carrriage\", ]\n",
    "\n",
    "energies_renouv = [\"solar (panel|energy)\", \"wind (power|energy)\", \"hydropower\",\n",
    "                   \"biofuel\", \"renewable (energy|resource)\", \"biomass\", \"firewood\", \"tidal energy\"]\n",
    "\n",
    "energies_fossiles = [\"fossil (energy|fuel)\", \" gas \", \" oil \",\n",
    "                     \"petroleum\", \" coal \", \" fuels \", \" hydrocarbons\"]\n",
    "\n",
    "performance = [\" horsepower\", \" record \", \" potency \", \" optimum \", \" maximum \", \" performance \", \"battery life\",\n",
    "               \"yield\", \"profitability\", \"cost-effectiveness\", \"financial viability\", \"investment\"]\n",
    "\n",
    "chimie = [\" chemi*\", \" substances\", \" elementary form\", \" matter \", \" atomic \", \"microscop*\", \"catalyst\",\n",
    "          \"colloid\", \"isomer\", \"reactant\", \"satureted\", \"solubility\", \"solvent\", \"(hetero|homo)geneous mixture\"]\n",
    "\n",
    "\n",
    "list_domains = [militaire, agriculture, astronautique, aéronautique, transport_march, transport_pers,\n",
    "                politique, automobile, ferroviaire, energies_renouv, energies_fossiles, performance, chimie]\n",
    "\n",
    "list_domains_BD = ['Militaire', 'Agriculture', 'Astronautique', \"Aéronautique\", \"Transport Marchandises\",\n",
    "                   \"Transport Personnes\", \"Politique\", \"Automobile\", \"Ferroviaire\", \"Energies Renouvelables\",\n",
    "                   \"Energies Fossiles\", \"Performance\", \"Chimie\"]\n",
    "\n",
    "\n",
    "df['Domaine'] = extraction_info2(\n",
    "    list_domains_BD, list_domains, title, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Titre</th>\n",
       "      <th>Langue</th>\n",
       "      <th>MotCle</th>\n",
       "      <th>Pays</th>\n",
       "      <th>Vehicule</th>\n",
       "      <th>Marque</th>\n",
       "      <th>Moteur</th>\n",
       "      <th>Stockage</th>\n",
       "      <th>Production</th>\n",
       "      <th>Domaine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6334032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hezlin Ashraf-Ball and Andrew J. Oswald and Ja...</td>\n",
       "      <td>Hydrogen Transport and the Spatial Requirement...</td>\n",
       "      <td>English</td>\n",
       "      <td>Renewable energy / wind power / land use / ene...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Gaz/</td>\n",
       "      <td></td>\n",
       "      <td>Energies Renouvelables/Energies Fossiles/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48223</td>\n",
       "      <td>2009-05-01</td>\n",
       "      <td>Andrew J. Oswald and James I. Oswald and Hezli...</td>\n",
       "      <td>Hydrogen transport and the spatial requirement...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Gaz/</td>\n",
       "      <td></td>\n",
       "      <td>Energies Renouvelables/Energies Fossiles/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52955835</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>PeiYuan Hsu and Xu Yang and Joshua L. Dibia an...</td>\n",
       "      <td>Design of Residential Hydrogen Fueling System ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faculty research day/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Gaz/</td>\n",
       "      <td></td>\n",
       "      <td>Automobile/Energies Renouvelables/Energies Fos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70657172</td>\n",
       "      <td>2008-09-24</td>\n",
       "      <td>Maria Antónia Travassos and A. I. Correia de S...</td>\n",
       "      <td>Penetration of hydrogen technologies: study on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Road transport-Portugal/ Pollutant emissions/ ...</td>\n",
       "      <td>Portugal/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Transport Personnes/Politique/Automobile/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40069044</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>Ivo Veldhuis</td>\n",
       "      <td>Application of hydrogen marine systems in high...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130 - Mechanical/ industrial/ civil and marine...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Liquide/</td>\n",
       "      <td></td>\n",
       "      <td>Aéronautique/Transport Marchandises/Transport ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        Date                                             Auteur  \\\n",
       "0   6334032         NaN  Hezlin Ashraf-Ball and Andrew J. Oswald and Ja...   \n",
       "1     48223  2009-05-01  Andrew J. Oswald and James I. Oswald and Hezli...   \n",
       "2  52955835  2011-01-01  PeiYuan Hsu and Xu Yang and Joshua L. Dibia an...   \n",
       "3  70657172  2008-09-24  Maria Antónia Travassos and A. I. Correia de S...   \n",
       "4  40069044  2007-01-01                                       Ivo Veldhuis   \n",
       "\n",
       "                                               Titre   Langue  \\\n",
       "0  Hydrogen Transport and the Spatial Requirement...  English   \n",
       "1  Hydrogen transport and the spatial requirement...  English   \n",
       "2  Design of Residential Hydrogen Fueling System ...      NaN   \n",
       "3  Penetration of hydrogen technologies: study on...      NaN   \n",
       "4  Application of hydrogen marine systems in high...      NaN   \n",
       "\n",
       "                                              MotCle       Pays Vehicule  \\\n",
       "0  Renewable energy / wind power / land use / ene...                       \n",
       "1                                                NaN                       \n",
       "2                              Faculty research day/                       \n",
       "3  Road transport-Portugal/ Pollutant emissions/ ...  Portugal/            \n",
       "4  130 - Mechanical/ industrial/ civil and marine...                       \n",
       "\n",
       "  Marque Moteur  Stockage Production  \\\n",
       "0                    Gaz/              \n",
       "1                    Gaz/              \n",
       "2                    Gaz/              \n",
       "3                                      \n",
       "4                Liquide/              \n",
       "\n",
       "                                             Domaine  \n",
       "0          Energies Renouvelables/Energies Fossiles/  \n",
       "1          Energies Renouvelables/Energies Fossiles/  \n",
       "2  Automobile/Energies Renouvelables/Energies Fos...  \n",
       "3          Transport Personnes/Politique/Automobile/  \n",
       "4  Aéronautique/Transport Marchandises/Transport ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The results are checked \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZpfG3Xv12jNj"
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a file \n",
    "df.to_csv(\"df_scraping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Scraping.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
