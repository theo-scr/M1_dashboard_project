{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Date :</strong> Created on 10 March 2021| Updated on 20 March 2021 </strong>\n",
    "\n",
    "<strong>Group 2 - Hydrogen vehicles \n",
    "    \n",
    "@author : </strong>Théo SACCAREAU\n",
    "\n",
    "<strong>insert_database_V0.4\n",
    "      \n",
    "Description :</strong> The purpose of this notebook is to insert the information recovered during scraping into the Oracle database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDFCWZM1F9q2"
   },
   "source": [
    "# Install / Download / Import Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8KNadDySF9q4"
   },
   "outputs": [],
   "source": [
    "# Text librairy\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Useful librairies\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "NbSK1wRYF9q-",
    "outputId": "520868d9-3b4e-4cc4-b73b-7f398d56cdbf"
   },
   "outputs": [],
   "source": [
    "# Connection to the database 'SCT2985A'.\n",
    "mydb = cx_Oracle.connect('SCT2985A/esg@//telline.univ-tlse3.fr:1521/etupre',\n",
    "                         encoding='UTF-8',\n",
    "                         nencoding=\"UTF-8\")\n",
    "\n",
    "# Creation of the cursor. It is used to execute statements to communicate with the Oracle database.\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 - Insertion of \"basic\" information. \n",
    "Insertion of information that does not require the content of the scraping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8GgK-kZOhTG9"
   },
   "outputs": [],
   "source": [
    "def insert(sql_request: str, list_items: list):\n",
    "    \"\"\" Documentation :\n",
    "            - Function allowing to make a data insertion request in the database. The primary key is an\n",
    "            identifier that is manually incremented at each insertion. \n",
    "\n",
    "        Parameters:\n",
    "            - sql_request : insertion request in SQL language.\n",
    "            - list_items : list of items to insert in the database. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization of the identifier\n",
    "    id = 0\n",
    "\n",
    "    # Browse the list\n",
    "    for item in tqdm(list_items):\n",
    "\n",
    "        # The values to be inserted are the identifier and the current value of the list (item).\n",
    "        values = [id, item]\n",
    "        # Execution of the request\n",
    "        mycursor.execute(sql_request, values)\n",
    "        # Incrementing of the identifier\n",
    "        id += 1\n",
    "\n",
    "    # Once all the insertions are done, we make a commit\n",
    "    mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1) Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Tw1ZEuvWF9rF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:24<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "list_countries_BD = [\"Algérie\", \"Angola\", \"Bénin\", \"Botswana\", \"Burkina Faso\", \"Burundi\", \"Cameroun\", \"Cap vert\",\n",
    "                     \"République centrafricaine\", \"Tchad\", \"Comores\", \"Congo\", \"Djibouti\", \"Egypte\",\n",
    "                     \"Guinée équatoriale\", \"Erythree\", \"Ethiopie\", \"Gabon\", \"Gambie\", \"Ghana\", \"Guinée\",\n",
    "                     \"Guinée-Bissau\", \"Côte d'Ivoire\", \"Kenya\", \"Lesotho\", \"Liberia\", \"Libye\", \"Madagascar\",\n",
    "                     \"Malawi\", \"Mali\", \"Mauritanie\", \"Ile Maurice\", \"Maroc\", \"Mozambique\", \"Namibie\", \"Niger\",\n",
    "                     \"Nigeria\", \"Rwanda\", \"Sao Tomé-et-Principe\", \"Sénégal\", \"Seychelles\", \"Sierra Leone\",\n",
    "                     \"Somalie\", \"Afrique du Sud\", \"Soudan\", \"Swaziland\", \"Tanzanie\", \"Togo\", \"Tunisie\", \"Ouganda\",\n",
    "                     \"Zambie\", \"Zimbabwe\", \"Albanie\", \"Andorre\", \"Arménie\", \"Autriche\", \"Azerbaijan\",\n",
    "                     \"Biélorussie\", \"Belgique\", \"Bosnie\", \"Bulgarie\", \"Croatie\", \"Chypre\", \"République Tchèque\",\n",
    "                     \"Danemark\", \"Estonie\", \"Finlande\", \"France\",\n",
    "                     \"Géorgie\", \"Allemagne\", \"Grèce\", \"Hongrie\", \"Islande\", \"Irelande\", \"Italie\", \"Lettonie\",\n",
    "                     \"Liechtenstein\", \"Lituanie\", \"Luxembourg\", \"Macédoine\", \"Malte\", \"Moldavie\", \"Monaco\",\n",
    "                     \"Montenegro\", \"Pays-Bas\", \"Norvège\", \"Pologne\", \"Portugal\", \"Roumanie\", \"Saint-Marin\",\n",
    "                     \"Serbie\", \"Slovaquie\", \"Slovénie\", \"Espagne\", \"Suède\", \"Suisse\", \"Ukraine\", \"Royaume-Uni\",\n",
    "                     \"Vatican\", \"Antigua-et-Barbuda\", \"Bahamas\", \"La Barbade\", \"Belize\", \"Canada\", \"Costa Rica\",\n",
    "                     \"Cuba\", \"Dominique\", \"République dominicaine\", \"El Salvador\", \"Grenade\", \"Guatemala\", \"Haiti\",\n",
    "                     \"Honduras\", \"Jamaique\", \"Mexique\", \"Nicaragua\", \"Paname\", \"Saint-Christophe et Niévès\",\n",
    "                     \"Saint Lucie\", \"Saint Vincent et les Grenadines\", \"Trinidad et Tobago\", \"Etats-Unis\",\n",
    "                     \"Argentine\", \"Bolivie\", \"Brésil\", \"Chili\", \"Colombie\", \"Equateur\", \"Guyane\", \"Paraguay\",\n",
    "                     \"Pérou\", \"Suriname\", \"Uruguay\", \"Venezuela\", \"Afghanistan\", \"Bahrein\", \"Bangladesh\",\n",
    "                     \"Bhoutan\", \"Brunei\", \"Myanmar\", \"Cambodge\", \"Chine\", \"Timor oriental\", \"Inde\", \"Indonésie\",\n",
    "                     \"Iran\", \"Irak\", \"Israël\", \"Japon\", \"Jordanie\", \"Kazakhstan\", \"Corée du Sud\", \"Corée du Nord\",\n",
    "                     \"Koweït\", \"Kyrgyzstan\", \"Laos\", \"Liban\", \"Malaysie\", \"Maldives\", \"Mongolie\", \"Népal\", \"Oman\",\n",
    "                     \"Pakistan\", \"Palestine\", \"Philippines\", \"Qatar\", \"Russie\", \"Arabie Saoudite\", \"Singapour\",\n",
    "                     \"Sri Lanka\", \"Syrie\", \"Tajikistan\", \"Thaïlande\", \"Turquie\", \"Turkmenistan\",\n",
    "                     \"Emirats Arabes Unis\", \"Ouzbekistan\", \"Viêtnam\", \"Yémen\", \"Australie\", \"Fiji\",\n",
    "                     \"République de Kiribati\", \"Iles Marshall\", \"Micronésie\", \"Nauru\", \"Nouvelle-Zélande\", \"Palau\",\n",
    "                     \"Papouasie Nouvelle Guinée\", \"Iles Samoa\", \"Iles Salomon\", \"Tonga\", \"Tuvalu\", \"Vanuatu\",\n",
    "                     \"République Democratique du Congo\"]\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Pays VALUES ( :1, :2)\"\n",
    "\n",
    "# Fonction 'insert' for the table \"Pays\"\n",
    "insert(sql_request, list_countries_BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2) Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rbKNM3P7HnXx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:09<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "list_brands_BD = [\"Hyundai\", \"Toyota\", \"Renault\", \"Honda\", \"Airbus\", \"Boeing\", \"Thalès\", \"Mercedes\", \"Audi\", \"Kia\",\n",
    "                  \"Riversimple\", \"Nissan\", \"Ford\", \"Daimler\", \"Alstom\", \"BMW\", \"Hopium\", \"Peugeot\", \"Volkswagen\",\n",
    "                  \"General Motors\", \"PSA\", \"Roland Gumpert\", \"Mazda\", \"Aston Martin\", \"Pininfarina\", \"Suzuki\",\n",
    "                  \"Volvo\", \"Opel\", \"Dassault\", \"Cessna\", \"Bombardier\", \"MiG\", \"Diamond Aircraft\", \"ZeroAvia\",\n",
    "                  \"Rolls-Royce\", \"Eviation\", \"Khrounitchev\", \"SpaceX\", \"Avio\", \"ArianeGroup\",\n",
    "                  \"United Launch Alliance\", \"McDonnell Douglas\", \"Mitsubishi Heavy Industries\", \"ISRO\",\n",
    "                  \"Ioujnoie\", \"Citroën\", \"Fiat\", \"Lancia\", \"Skoda\", \"Yamaha\", \"KTM\", \"Kawasaki\", \"Ducati\",\n",
    "                  \"Suzuki\"]\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Marque VALUES ( :1, :2)\"\n",
    "\n",
    "# Fonction 'insert' for the table \"Marque\"\n",
    "insert(sql_request, list_brands_BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3) Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IigkDQoShFay"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 23.39it/s]\n"
     ]
    }
   ],
   "source": [
    "list_domains_BD = [\"Militaire\", \"Agriculture\", \"Astronautique\", \"Aéronautique\", \"Transport Marchandises\",\n",
    "                   \"Transport Personnes\", \"Politique\", \"Automobile\", \"Ferroviaire\", \"Energies Renouvelables\",\n",
    "                   \"Energies Fossiles\", \"Performance\", \"Chimie\"]\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Domaine VALUES ( :1, :2)\"\n",
    "\n",
    "# Fonction 'insert' for the table \"Domaines\"\n",
    "insert(sql_request, list_domains_BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4) Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nEPT7wWAh7Cz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 23.51it/s]\n"
     ]
    }
   ],
   "source": [
    "list_vehicles_BD = [\"Train\", \"Tracteur\", \"Vélo\", \"Bateau\", \"Avion\", \"Voiture\", \"Camion\", \"Bus\", \"Fusée\",\n",
    "                     \"Hélicoptère\", \"Moto\", \"Tramway\", \"Sous-marin\"]\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Vehicule VALUES ( :1, :2)\"\n",
    "\n",
    "# Fonction 'insert' for the table \"Vehicules\"\n",
    "insert(sql_request, list_vehicles_BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5) Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pIUlMAJ4h7Gc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "list_moteur_BD = [\"Combustion interne\",\n",
    "                  \"Réaction\",  \"Pile à combustible\", \"Hybride\"]\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Moteur  VALUES ( :1, :2)\"\n",
    "\n",
    "# Fonction 'insert' for the table \"Moteur\"\n",
    "insert(sql_request, liste_moteur_BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhLnTA53_E2-"
   },
   "source": [
    "## 1-6) Technologies\n",
    "Table different from the previous ones: there are 4 attributes. So we cannot use the 'insert' function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y1USNNk0h6_8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Several lists to differentiate technologies (storage or production and type of production).\n",
    "# This will allow the right values to be set for the attributes.\n",
    "list_type_prod_BD_H = [\"SMR\", \"Pyrolyse du méthane\", \"Oxydation partielle\", \"Reformage plasma\",\n",
    "                       \"Gazéification du charbon\"]\n",
    "\n",
    "list_type_prod_BD_E = [\"Electrolyse\", \"Electrochimie\", \"Radiolyse\", \"Thermolyse\", \"Thermochimie\",\n",
    "                       \"Ferrosilicium\", \"Culture d'algues\", \"Fission photocatalytique\"]\n",
    "\n",
    "list_type_prod_BD_A = [\"Fermentation\", \"Production enzymatique\",\n",
    "                       \"Electrolyse biocatalysée\"]\n",
    "\n",
    "list_storage_BD = ['Gaz', 'Liquide', 'Solide']\n",
    "\n",
    "\n",
    "# SQL request\n",
    "sql_request = \"INSERT INTO Technologie VALUES ( :1, :2, :3, :4)\"\n",
    "\n",
    "# Initialization of identifiant and cumpter\n",
    "id_techno = 0\n",
    "cpt = 0\n",
    "\n",
    "# The 4 types of lists are browsed\n",
    "for list_tech in tqdm([liste_type_prod_BD_H, liste_type_prod_BD_E, liste_type_prod_BD_A, liste_stockage_BD]):\n",
    "\n",
    "    # If cpt is not equal to 3, it's a production technology\n",
    "    if (cpt != 3):\n",
    "        cat = 'Production'\n",
    "    # Else it's a storage technology\n",
    "    else:\n",
    "        cat = 'Stockage'\n",
    "\n",
    "    # If cpt is equal to 0, the source is Hydraucarbon\n",
    "    if (cpt == 0):\n",
    "        source = 'Hydraucarbures'\n",
    "    # If cpt equals 1, the source is Water\n",
    "    elif (cpt == 1):\n",
    "        source = 'Eau'\n",
    "    # If cpt is equal to 2, the source is Other\n",
    "    elif (cpt == 2):\n",
    "        source = 'Autre'\n",
    "    # Else there is no source.\n",
    "    else:\n",
    "        source = None\n",
    "\n",
    "    # The technology list is browsed and inserted into the BD\n",
    "    for techno in list_tech:\n",
    "        values = [id_techno, techno, cat, source]\n",
    "        mycursor.execute(sql_request, values)\n",
    "        id_techno += 1\n",
    "\n",
    "    # Incrementation of cpt since we change of list\n",
    "    cpt += 1\n",
    "\n",
    "# Once all the insertions are done, we make a commit\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 - Insertion of \"scraping\" information. \n",
    "Insertion of information that require the content of the scraping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8439, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information contained in the 'df_scraping.csv' file (created with the 'scraping_v1.4' notebook) is\n",
    "# recovered in a dataframe.\n",
    "df = pd.read_csv(\"../scraping/df_scraping.csv\",\n",
    "                 sep=',', index_col=False, encoding='utf-8')\n",
    "\n",
    "# Check the dimensions of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1) Authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IsuAv9Kwh7Jk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [16:23<00:00,  8.58it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Initialization of useful variables.\n",
    "sql_request_authors = \"INSERT INTO Auteur  VALUES ( :1, :2)\"  # SQL requests\n",
    "id_authors = 0  # authors identifier\n",
    "\n",
    "# Set to avoid reinserting an author several times\n",
    "authors_already_inserted = set()\n",
    "\n",
    "# Dictionary to find the names of authors. For example, in the database, \"오진숙\" will be coded as\n",
    "# \"\\xec\\x98\\xa4\\xec\\xa7\\x84\\xec\\x88\\x99\". Thanks to this dictionary, we will be able to find the real name\n",
    "# of the authors with special characters when viewing the data.\n",
    "author_utf8 = dict()\n",
    "\n",
    "# Browse the list of authors\n",
    "for authors in tqdm(df['Auteur']):\n",
    "\n",
    "    # If the information for the current article is not None\n",
    "    if (authors is not np.nan):\n",
    "\n",
    "        # Separate authors with the 'and' separation (see the Scraping Notebook)\n",
    "        authors = authors.split(' and ')\n",
    "\n",
    "        # Cleaning up names :\n",
    "        # - Remove numbers and special characters\n",
    "        # - Put everything in lower case, then put only the first letter in upper case\n",
    "        # - Everything in brackets is removed\n",
    "        # - Finally, we remove the spaces before and after.\n",
    "        authors = [re.sub(\"([0-9]|(\\((.*?)\\))|\\-|\\(|\\)|#|\\?|&|\\[|\\]|�|!)\",\n",
    "                          \"\", author).lower().strip().title() for author in auteurs]\n",
    "\n",
    "        # Extra spaces between names are removed\n",
    "        authors = [' '.join(author.split()) for author in authors]\n",
    "\n",
    "        # Browse all authors who have written the current article\n",
    "        for author in authors:\n",
    "\n",
    "            # We check that it has not already been inserted and that the length of its name\n",
    "            # is not just a character or an empty string.\n",
    "            if author not in authors_already_inserted and len(author) > 2:\n",
    "                # It is added to the set\n",
    "                authors_already_inserted.add(author)\n",
    "\n",
    "                # We try to encode it in 'Ascii', if we can't do it, it contains special characters.\n",
    "                # It must therefore be encoded with the 'UTF-8' format and added to the dictionary (see\n",
    "                # explanation above).\n",
    "                try:\n",
    "                    author_BD = str(author.encode('ascii'))[2:-1]\n",
    "                except:\n",
    "                    author_BD = str(author.encode())[2:-1]\n",
    "                    author_utf8[auteur] = author_BD\n",
    "\n",
    "                # The values to be inserted are the identifier and the name of auteur.\n",
    "                values = [id_authors, author_BD]\n",
    "                # Execution of the request\n",
    "                mycursor.execute(sql_request_authors, values)\n",
    "                # Incrementing of the identifier\n",
    "                id_authors += 1\n",
    "\n",
    "# Once all the insertions are done, we make a commit\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2) Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, a function will be needed to clean up the dates and have a uniform format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer(date: str) -> str:\n",
    "    \"\"\"\n",
    "        Documentation :\n",
    "            - Function that standardises the format of dates. \n",
    "\n",
    "        Parameter :\n",
    "            - date : date in raw format. \n",
    "\n",
    "        Output :\n",
    "            - date_clean : date in 'day/month/year' format. \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the date is in 'str' format (i.e. that it is not None)\n",
    "    if (type(date) == str):\n",
    "\n",
    "        # Most of the scraped dates are in 'year-month-day' format.\n",
    "        # Simply retrieve each element and put it back in the order that we want.\n",
    "        if (len(date) == 10 and date[4] == '-' and date[7] == '-'):\n",
    "            annee = date[:4]\n",
    "            mois = date[5:7]\n",
    "            jour = date[-2:]\n",
    "\n",
    "            # Sometimes there is no day and/or month and the value is 00.\n",
    "            # If this is the case, we change it to 01 (we will do our analysis on years only)\n",
    "            if (int(mois) < 1 or int(mois) > 12):\n",
    "                mois = '01'\n",
    "            if (int(jour) < 1 or int(jour) > 31):\n",
    "                jour = '01'\n",
    "\n",
    "            date_clean = jour + '-' + mois + '-' + annee\n",
    "\n",
    "        # Another format is \"[year]\". Just get the year in square brackets and put 01 for the day and month\n",
    "        elif (len(date) == 6 and date[0] == '[' and date[-1] == ']'):\n",
    "            annee = date[1:-1]\n",
    "            date_clean = '01-01-' + annee\n",
    "\n",
    "        # If the date is of length 4 and contains 19** or 20**, it is a year.\n",
    "        elif (len(date) == 4 and ('19' in date or '20' in date)):\n",
    "            date_clean = '01-01-' + date\n",
    "\n",
    "        # If it is another format than the three above, we just look if there is a year present (19** or 20**).\n",
    "        elif (re.search(\"(19|20)[0-9]{2}\", date) is not None):\n",
    "            pos = re.search(\"(19|20)[0-9]{2}\", date).start()\n",
    "            annee = date[pos:pos+4]\n",
    "            date_clean = '01-01-' + annee\n",
    "            \n",
    "        # Otherwise we can't determine the date, we return None \n",
    "        else:\n",
    "            date_clean = None\n",
    "    else:\n",
    "        date_clean = None\n",
    "\n",
    "    # date in 'day/month/year' format (or None)\n",
    "    return date_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_2w6QwPk-hcR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [06:48<00:00, 20.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "# The previous function is applied to each date.\n",
    "dates = df['Date'].apply(lambda x: nettoyer(x))\n",
    "\n",
    "# For all titles, we put them all in lower case except the first letter (to standardize)\n",
    "titles = df['Titre'].apply(lambda x: None if (\n",
    "    x is np.nan) else x.lower().capitalize())\n",
    "\n",
    "# For languages, np.nan and 'Undetermined' are replaced by None\n",
    "langages = df['Langue'].apply(lambda x: None if x == 'Undetermined' else x).apply(\n",
    "    lambda x: None if (x is np.nan) else x)\n",
    "\n",
    "# SQL request \n",
    "sql_request_articles = \"INSERT INTO Article  VALUES ( :1, :2, :3, :4)\"\n",
    "\n",
    "# Note: a 'drop duplicate' has been performed beforehand => no risk of duplication\n",
    "for id, title, langage, date in zip(tqdm(df['Id']), titles, langages, dates):\n",
    "    \n",
    "    # The values to be inserted are the identifier, the title, the langage and the date.\n",
    "    values = [id, title, langage, date]\n",
    "\n",
    "    # Execution of the request\n",
    "    mycursor.execute(sql_request_articles, values)\n",
    "\n",
    "# Once all the insertions are done, we make a commit\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3) Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanning(item: str, special_character: list):\n",
    "    \"\"\"\n",
    "        Documentation :\n",
    "            - Function that cleans up keywords. \n",
    "\n",
    "        Parameters :\n",
    "            - item (str) : keyword to clean. \n",
    "            - special_character (list) : a list of specials characters to remove to\n",
    "            the keywords. \n",
    "\n",
    "        Output :\n",
    "            - result : cleaned keyword without unnecessary characters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    item: str = item.lower()\n",
    "\n",
    "    # Everything in brackets is removed \n",
    "    iteam: str = re.sub(\"((\\((.*?)\\))|)\", \"\", item)\n",
    "\n",
    "    # Remove accents\n",
    "    item = unicodedata.normalize('NFD', str(item)).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "    # Remove special character\n",
    "    for i in special_character:\n",
    "\n",
    "        item = item.replace(i, \"\")\n",
    "\n",
    "    # Remove whitespaces\n",
    "    item = item.strip()\n",
    "    result = ' '.join(item.split())\n",
    "\n",
    "    # Return cleaned keyword\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of special characters\n",
    "special_character: list = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"\\\\\", \"(\",\n",
    "                           \")\", \"*\", \"+\", \",\", \"-\", \".\", \":\", \";\",\n",
    "                           \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\",\n",
    "                           \"{\", \"|\", \"}\", \"~\", \"«\", \"»\", \"’\", \"•\", \"…\",\n",
    "                           \"â\", \"€\", \"™\", \"—\", \"�\", \"–\", \"“\", \"”\"]\n",
    "\n",
    "# Cleanning of each keywords. \n",
    "col_keywords = df['MotCle'].apply(\n",
    "    lambda x: x if x is np.nan else cleanning(x, special_character))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       renewable energy / wind power / land use / ene...\n",
       "1                                                     NaN\n",
       "2                                   faculty research day/\n",
       "3       road transportportugal/ pollutant emissions/ c...\n",
       "4       130 mechanical/ industrial/ civil and marine e...\n",
       "                              ...                        \n",
       "8434                                                  NaN\n",
       "8435                                                  NaN\n",
       "8436                                                  NaN\n",
       "8437                                                  NaN\n",
       "8438                                                  NaN\n",
       "Name: MotCle, Length: 8439, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the result \n",
    "col_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [04:44<00:00, 29.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Initialization of useful variables.\n",
    "id_keyword = 0 # keyword identifier\n",
    "sql_request = \"INSERT INTO MotCle  VALUES ( :1, :2)\" # SQL request\n",
    "\n",
    "# Set to avoid reinserting an author several times\n",
    "keywords_already_inserted = set()\n",
    "\n",
    "# The list of keywords for each article is browsed\n",
    "for keywords in tqdm(col_keywords):\n",
    "    \n",
    "    # If the keywords for the current article is not None\n",
    "    if (keywords is not np.nan):\n",
    "        \n",
    "        # Separate keywords with the '/' separation (see the Scraping Notebook)\n",
    "        keywords = keywords.split('/')\n",
    "\n",
    "        # Removing excess space \n",
    "        keywords = [' '.join(words.split()) for words in keywords]\n",
    "\n",
    "        # Each word in the keyword list is browsed \n",
    "        for word in keywords:\n",
    "            \n",
    "            # The first letter is capitalized \n",
    "            word = word.capitalize()\n",
    "\n",
    "            # We check that it has not already been inserted and that the length of word\n",
    "            # is the size is between 3 and 150 characters \n",
    "            if word not in keywords_already_inserted and len(mot) > 2 and len(word) < 150:\n",
    "                \n",
    "                # It is added to the set\n",
    "                keywords_already_inserted.add(word)\n",
    "                # The values to be inserted are the identifier and the word. \n",
    "                values = [id_keyword, word]\n",
    "                # Execution of the request\n",
    "                mycursor.execute(sql_request, values)\n",
    "                # Incrementing of the identifier\n",
    "                id_keyword += 1\n",
    "\n",
    "# Once all the insertions are done, we make a commit\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4) Countries mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert2(relation : str, col : str, att : str, name_id_att : str):\n",
    "    \"\"\"\n",
    "        Documentation :\n",
    "            - Function that makes a data insertion request in the database. \n",
    "            \n",
    "        Parameters :\n",
    "            - relation (str) : name of the relationship/table  \n",
    "            - col (str) : name of the column in the dataframe  \n",
    "            - att (str) : name of the attribute \n",
    "            - name_id_att (str) : name of the identifier \n",
    "    \"\"\"\n",
    "    \n",
    "    # SQL request\n",
    "    sql_request = \"INSERT INTO \" + relation + \"_Evoq VALUES ( :1, :2)\"\n",
    "    \n",
    "    # The dataframe is browsed, retrieving the item ID and the value of the relevant column\n",
    "    for chaine_att, id in zip(tqdm(df[col]), df['Id']):\n",
    "\n",
    "        # If the value for the current article is not None\n",
    "        if (chaine_att is not np.nan):\n",
    "            \n",
    "            # Separate keywords with the '/' separation (see the Scraping Notebook)\n",
    "            list_att = chaine_att.split('/')\n",
    "            \n",
    "            # Only elements whose length is greater than 1 are kept (to avoid empty strings)\n",
    "            list_att = [item for item in list_att if len(item) > 1]\n",
    "\n",
    "            # Each item in the list is browsed \n",
    "            for item in list_att:\n",
    "                \n",
    "                # Double the apostrophes for the SQL query to work\n",
    "                item = item.replace(\"'\", \"''\")\n",
    "                \n",
    "                # SELECT SQL request to retrieve identifiers  \n",
    "                sql_request_select = \"SELECT \" + name_id_att + \" FROM \" + \\\n",
    "                    relation + \" WHERE \" + att + \" = '\" + item + \"'\"\n",
    "                \n",
    "                # Execution of the SELECT request \n",
    "                res = mycursor.execute(sql_request_select)\n",
    "                \n",
    "                # We extract the identifiers \n",
    "                id_att = [row[0] for row in res][0]\n",
    "                \n",
    "                # The values to be inserted are the identifier of articles and the identifer of current relation\n",
    "                values = [id, id_att]\n",
    "                \n",
    "                # Execution of the INSERT request \n",
    "                mycursor.execute(sql_request, values)\n",
    "\n",
    "    # Once all the insertions are done, we make a commit\n",
    "    mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [03:54<00:00, 36.04it/s] \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Pays\", \"Pays\", \"nom_pays\", \"id_pays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-5) Engines mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [11:51<00:00, 11.87it/s]  \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Moteur\", \"Moteur\", \"type_moteur\", \"id_moteur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-6) Vehicles mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [04:11<00:00, 33.50it/s] \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Vehicule\", \"Vehicule\", \"type_vehicule\", \"id_vehicule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-7) Domains mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [27:49<00:00,  5.06it/s] \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Domaine\", \"Domaine\", \"nom_domaine\", \"id_domaine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-8) Brands mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [00:44<00:00, 188.26it/s]\n"
     ]
    }
   ],
   "source": [
    "insert2(\"Marque\", \"Marque\", \"nom_marque\", \"id_marque\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-9) Storages mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [04:00<00:00, 35.02it/s] \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Technologie\", \"Stockage\", \"nom_tech\", \"id_tech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-10) Type of production mentioned in the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [03:08<00:00, 44.77it/s] \n"
     ]
    }
   ],
   "source": [
    "insert2(\"Technologie\", \"Production\", \"nom_tech\", \"id_tech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-11) Keywords mentioned in the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8439/8439 [15:41<00:00,  8.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "# SQL request \n",
    "sql_request = \"INSERT INTO MotCle_Evoq VALUES ( :1, :2)\"\n",
    "\n",
    "# Note : We browse 'col_keywords' so the keywords are already cleaned\n",
    "for keywords, id in zip(tqdm(col_keywords), df['Id']):\n",
    "\n",
    "    # If the keywords for the current article is not None\n",
    "    if (keywords is not np.nan):\n",
    "        \n",
    "        # Separate keywords with the '/' separation (see the Scraping Notebook)\n",
    "        keywords = keywords.split('/')\n",
    "        \n",
    "        # Only words whose length is greater than 1 are kept (to avoid empty strings) \n",
    "        keywords = [word for word in keywords if len(word) > 1]\n",
    "\n",
    "        # Removing excess space \n",
    "        keywords = [' '.join(item.split()) for item in keywords]\n",
    "\n",
    "        # Tip to remove possible duplicates (and avoid the duplicate primary key problem)\n",
    "        keywords = list(set(keywords))\n",
    "\n",
    "        # Each word in the keyword list is browsed \n",
    "        for word in keywords:\n",
    "            \n",
    "            # The first letter is capitalized \n",
    "            word = word.capitalize()\n",
    "            \n",
    "            # We check that the length of word is the size is between 3 and 150 characters \n",
    "            if len(word) > 2 and len(word) < 150:\n",
    "                \n",
    "                # Double the apostrophes for the SQL query to work\n",
    "                word = word.replace(\"'\", \"''\")\n",
    "                \n",
    "                # SELECT SQL request to retrieve identifiers  \n",
    "                sql_request_select = \"SELECT id_motcle FROM MotCle WHERE mot = '\" + word + \"'\"\n",
    "                # Execution of the SELECT request \n",
    "                res = mycursor.execute(sql_request_select)\n",
    "                # We extract the identifiers \n",
    "                id_att = [row[0] for row in res][0]\n",
    "                \n",
    "                # The values to be inserted are the identifier of articles and the identifer of current wordkey\n",
    "                values = [id, id_att]\n",
    "                # Execution of the INSERT request \n",
    "                mycursor.execute(sql_request, values)\n",
    "\n",
    "# Once all the insertions are done, we make a commit\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [23:54<00:00,  2.79it/s]  \n",
      "100%|██████████| 4439/4439 [27:26<00:00,  2.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "# SQL request \n",
    "sql_request = \"INSERT INTO Auteur_Evoq VALUES ( :1, :2)\"\n",
    "\n",
    "# We have to split the execution of this program in two otherwise the maximum number \n",
    "# of requests is reached (between SELECT and INSERT request) \n",
    "authors1 = df[\"Auteur\"].iloc[:4000]\n",
    "id1 = df['Id'].iloc[:4000]\n",
    "authors2 = df[\"Auteur\"].iloc[4000:]\n",
    "id2 = df['Id'].iloc[4000:]\n",
    "\n",
    "for col_author, col_id in zip([authors1, authors2], [id1, id2]):\n",
    "    \n",
    "    # The dataframe is browsed, retrieving the item ID and the value of the relevant column\n",
    "    for authors, id in zip(tqdm(col_author), col_id):\n",
    "\n",
    "        # If the list of authors for the current article is not None\n",
    "        if (authors is not np.nan):\n",
    "            \n",
    "            # Separate keywords with the ' and ' separation (see the Scraping Notebook)\n",
    "            authors = authors.split(' and ')\n",
    "            \n",
    "            # Cleaning up names :\n",
    "            # - Remove numbers and special characters\n",
    "            # - Put everything in lower case, then put only the first letter in upper case\n",
    "            # - Everything in brackets is removed\n",
    "            # - Finally, we remove the spaces before and after.\n",
    "            authors = [re.sub(\"([0-9]|(\\((.*?)\\))|\\-|\\(|\\)|#|\\?|&|\\[|\\]|�|!)\",\n",
    "                                \"\", author).lower().strip().title() for author in authors]\n",
    "            \n",
    "            # Extra spaces between names are removed. \n",
    "            authors = [' '.join(author.split()) for author in authors]\n",
    "            \n",
    "            # Only words whose length is greater than 2 are kept (to avoid empty strings or errors) \n",
    "            authors = [author for author in authors if len(author) > 2]\n",
    "\n",
    "            # Tip to remove possible duplicates (and avoid the duplicate primary key problem)\n",
    "            authors = list(set(authors))\n",
    "\n",
    "            # Browse all authors who have written the current article\n",
    "            for author in authors:\n",
    "                \n",
    "                # We try to encode it in 'Ascii', if we can't do it, it contains special characters.\n",
    "                # It must therefore be encoded with the 'UTF-8' format.\n",
    "                try:\n",
    "                    author = str(author.encode('ascii'))[2:-1]\n",
    "\n",
    "                except:\n",
    "                    author = str(author.encode())[2:-1]\n",
    "                \n",
    "                \n",
    "                # Double the apostrophes for the SQL query to work\n",
    "                author = author.replace(\"'\", \"''\")\n",
    "\n",
    "                # SELECT SQL request to retrieve identifiers  \n",
    "                sql_request_select = \"SELECT id_auteur FROM Auteur WHERE nom_prenom = '\" + author + \"'\"\n",
    "                # Execution of the SELECT request \n",
    "                res = mycursor.execute(sql_request_select)\n",
    "                # We extract the identifiers \n",
    "                id_att = [row[0] for row in res][0]\n",
    "                \n",
    "                # The values to be inserted are the identifier of articles and the identifer of current author\n",
    "                values = [id, id_att]\n",
    "                # Execution of the INSERT request \n",
    "                mycursor.execute(sql_request, values)\n",
    "\n",
    "    # On commit\n",
    "    mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We close the connection to the database \n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "insertion_BD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
